# Linear-Regression-Predicting-Average-GPU-Running-Time
As a part of this project, I have worked on

• The implementation of the gradient descent algorithm to find out the optimal values for the
coefficients of the explanatory variables in the dataset.

• Applied Linear Regression to find out the average computation time. Further, converted the problem statement as a classification problem to find out whether the average GPU computation time is low or high and applied Support Vector Machine (SVM), Decision Trees and XGBoost models to classify the average running time.

• Plotting learning curves (error metrics with respect to iterations, learning rate, and convergence threshold values).

• Interpretation of the plots and results.
